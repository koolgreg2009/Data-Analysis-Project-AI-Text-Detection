# -*- coding: utf-8 -*-
"""Copy of a4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HAGd5G6dTQa24inIJAqgMH4Aa2P0AcSv

## Part I - Sentiment Analysis
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk import pos_tag, word_tokenize
from wordcloud import WordCloud
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree
from collections import Counter
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

from google.colab import drive
drive.mount('/content/drive')

"""##### Imports"""

train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/covid-tweets-train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/covid-tweets-test.csv')
news_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/news_train.csv')
val_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/validation.csv')
test_01 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/test_1.csv')
test_02 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/test_2.csv')
test_03 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/test_3.csv')
test_04 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/test_4.csv')
test_05 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/JSC270/Assignments/a4_Jeremy_Kevin/test_5.csv')

news_df = pd.read_parquet("/content/drive/MyDrive/Assignment 4/train-00000-of-00001.parquet")
news_df = news_df.drop("image", axis=1)
news_df.to_csv('/content/drive/MyDrive/Assignment 4/news_train.csv', index=False)

news_df.head()

print(news_df['text'][0])

"""## Data Cleaning"""

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english')[:300])

def part2_preprocess(text):
    text = text.lower()

    text = re.sub(r'http\S+', '', text)

    text = re.sub(r'[^a-z0-9\s]', '', text)

    tokens = text.split()

    processed = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]

    return ' '.join(processed)

news_df['processed_text'] = news_df['text'].apply(part2_preprocess)
val_df['processed_text'] = test_05['text'].apply(part2_preprocess)
test_01['processed_text'] = test_01['text'].apply(part2_preprocess)
test_02['processed_text'] = test_02['text'].apply(part2_preprocess)
test_03['processed_text'] = test_03['text'].apply(part2_preprocess)
test_04['processed_text'] = test_04['text'].apply(part2_preprocess)
test_05['processed_text'] = test_05['text'].apply(part2_preprocess)

news_df[['text', 'processed_text']].head()

news_df

test_05

"""## Word clouds"""

# join all the processed text into one big string
text_data = " ".join(news_df['text'])


# Counter counts to feed WordCloud
word_freq = Counter(text_data.split())
wordcloud = WordCloud(width=800, height=400, background_color='white',
                      colormap='rainbow').generate_from_frequencies(word_freq)

# plot
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most Common Words before processing", fontsize=40, weight='bold')
plt.show()

text_data # as u can see this is actually pre-processed data with punctuations, stop words and etc, despite not a single stop word and punctuation being
# high frequency vocab



# join all the processed text into one big string
text_data = " ".join(news_df['processed_text'])

# create the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='rainbow').generate(text_data)

# display it
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most Common Words in Processed Text", fontsize=40, weight='bold')
plt.show()

all_ai = news_df[news_df['label'] == 1]
all_ai_text_data = " ".join(all_ai['processed_text'])

all_human = news_df[news_df['label'] == 0]
all_human_text_data = " ".join(all_human['processed_text'])



wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='rainbow').generate(all_ai_text_data)

# display it
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most Common Words in AI News", fontsize=40, weight='bold')
plt.show()

wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='rainbow').generate(all_human_text_data)

# display it
plt.figure(figsize=(15, 7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most Common Words in Non-AI News", fontsize=40, weight='bold')
plt.show()

news_df[news_df['processed_text'].str.contains("wednesday")]

print(news_df['text'][22])

# combine all tokens
#all_words = " ".join(news_df['processed_text']).split()
word_freq = Counter(text_data.split())

# top 20
common_words = pd.DataFrame(word_freq.most_common(15), columns=['word', 'count'])

sns.barplot(x='count', y='word', data=common_words)
plt.title("Top 15 Most Common Words")
plt.show()

"""## Barplot of class distribution"""

sns.countplot(x='label', data=news_df)
plt.title("Label Distribution")
plt.xlabel("Label (0 = Non-AI, 1 = AI)")
plt.ylabel("Count")
plt.show()

"""## Distribution of text length by classification (Historgram + KDE)"""

news_df['text_len'] = news_df['processed_text'].apply(lambda x: len(x.split()))

sns.histplot(data=news_df, x='text_len', hue='label', bins=30, kde=True)
plt.title("Text Length Distribution by Label")
plt.xlabel("Number of Words")
plt.show()

summary_stats = news_df.groupby('label')['text_len'].agg(['mean', 'std'])
print(summary_stats)

news_df

"""## Setting up validation df"""

val_X = val_df['processed_text']
val_y = val_df['label']

"""## Naive Bayesian Model"""

X_baye_train = news_df['processed_text']
y_baye_train = news_df['label']

tfidf = TfidfVectorizer(max_features=10000)
X_train_tfidf = tfidf.fit_transform(X_baye_train)

model_baye = MultinomialNB()
model_baye.fit(X_train_tfidf, y_baye_train)

val_X = val_df['processed_text'].fillna("")  # replace NaNs with empty string
val_X_tfidf = tfidf.transform(val_X)  # use the same vectorizer

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, recall_score

for alpha in [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]:
    model = MultinomialNB(alpha=alpha)
    model.fit(X_train_tfidf, y_baye_train)

    val_X = val_df['processed_text'].fillna("")
    val_X_tfidf = tfidf.transform(val_X)
    val_y = val_df['label']

    val_preds = model.predict(val_X_tfidf)

    acc = accuracy_score(val_y, val_preds)
    recall = recall_score(val_y, val_preds, pos_label=1)  # AI class = 1

    print(f"alpha = {alpha:.2f} â†’ accuracy = {acc:.4f}, recall (AI) = {recall:.4f}")

val_preds = model_baye.predict(val_X_tfidf)
print("Validation Accuracy:", accuracy_score(val_y, val_preds))
print("\nClassification Report:\n", classification_report(val_y, val_preds, digits=3))
print("\nConfusion Matrix:\n", confusion_matrix(val_y, val_preds))

model_baye = MultinomialNB(alpha=0.01)
model_baye.fit(X_train_tfidf, y_baye_train)

"""# Testing"""

X_test = test_05['processed_text']
X_test_tfidf = tfidf.transform(X_test)  # use the same vectorizer (donâ€™t fit again)

predictions = model_baye.predict(X_test_tfidf)
predictions

"""## Random Forest with TF-IDF vectorization"""

X_forest_train = news_df['processed_text']
y_forest_train = news_df['label']

tfidf = TfidfVectorizer(max_features=10000)
X_forest_train_tfidf = tfidf.fit_transform(X_forest_train)

rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=15,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)


rf_model.fit(X_forest_train_tfidf, y_forest_train)

val_preds = rf_model.predict(val_X_tfidf)
print("Validation Accuracy:", accuracy_score(val_y, val_preds))
print("\nClassification Report:\n", classification_report(val_y, val_preds, digits=3))
print("\nConfusion Matrix:\n", confusion_matrix(val_y, val_preds))

"""## Finding the parameter combination that yields the highest"""

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 15, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 'log2']
}

grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=3,         # reduce to 3-fold CV to save time
    n_jobs=-1,    # parallel processing
    verbose=2
)

grid.fit(X_forest_train_tfidf, y_forest_train)
print("Best parameters:", grid.best_params_)

rf_model_best = RandomForestClassifier(
    n_estimators=200,
    max_depth=20,
    max_features='log2',
    min_samples_leaf=2,
    min_samples_split=5,
    random_state=42
)
rf_model_best.fit(X_forest_train_tfidf, y_forest_train)

val_preds = rf_model_best.predict(val_X_tfidf)
print("Validation Accuracy:", accuracy_score(val_y, val_preds))
print("\nClassification Report:\n", classification_report(val_y, val_preds, digits=3))
print("\nConfusion Matrix:\n", confusion_matrix(val_y, val_preds))

"""### Plotting a selected tree"""

estimator = rf_model_best.estimators_[0]  # get first tree from the forest

plt.figure(figsize=(20, 10))
plot_tree(
    estimator,
    max_depth=3,                 # you can increase this, but it'll get crowded
    feature_names=tfidf.get_feature_names_out(),
    class_names=["Human", "AI"],
    filled=True,
    fontsize=10
)
plt.title("Decision Tree 0 from Random Forest", fontsize=16, fontweight="bold")
plt.show()

"""## The root node is checking:
ðŸ‘‰ Does the TF-IDF value of the word "campaigned" â‰¤ 0.137?
TF-IDF tells us how "important" the word "campaigned" is in a given document (based on frequency and rarity across all documents)

"""



# Naive Bayes predictions
nb_preds = model_baye.predict(val_X_tfidf)


# Random Forest predictions
rf_preds = rf_model_best.predict(val_X_tfidf)

# Naive Bayes Confusion Matrix
nb_cm = confusion_matrix(val_y, nb_preds)
disp_nb = ConfusionMatrixDisplay(confusion_matrix=nb_cm, display_labels=["Human", "AI"])
disp_nb.plot(cmap='Greens')
plt.title("Naive Bayes Confusion Matrix")
plt.show()

# Random Forest Confusion Matrix
rf_cm = confusion_matrix(val_y, rf_preds)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=rf_cm, display_labels=["Human", "AI"])
disp_rf.plot(cmap='Blues')
plt.title("Random Forest Confusion Matrix")
plt.show()



"""## Process and test on test dfs"""

def evaluate_models_on_test(test_df, model_baye, rf_model, tfidf, name="Test Set"):
    print(f"\nðŸ“Š Evaluating on {name}...")

    # Preprocessing
    X = test_df['processed_text'].fillna("")
    y = test_df['label']
    X_tfidf = tfidf.transform(X)

    # Naive Bayes
    nb_preds = model_baye.predict(X_tfidf)
    nb_acc = accuracy_score(y, nb_preds)
    print(f"\nðŸ”¹ Naive Bayes Accuracy: {nb_acc:.4f}")
    print(classification_report(y, nb_preds, target_names=["Human", "AI"]))

    # Random Forest
    rf_preds = rf_model.predict(X_tfidf)
    rf_acc = accuracy_score(y, rf_preds)
    print(f"\nðŸ”¹ Random Forest Accuracy: {rf_acc:.4f}")
    print(classification_report(y, rf_preds, target_names=["Human", "AI"]))

test_sets = {
    "test_01": test_01,
    # "test_02": test_02,
    # "test_03": test_03,
    "test_02": test_04,
    "test_03": test_05
}

for name, df in test_sets.items():
    evaluate_models_on_test(df, model_baye, rf_model_best, tfidf, name)

def plot_conf_matrices(test_sets, model_baye, rf_model, tfidf):
    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 25))

    for i, (name, df) in enumerate(test_sets.items()):
        # Preprocess
        X = df['processed_text'].fillna("")
        y = df['label']
        X_tfidf = tfidf.transform(X)

        # Naive Bayes
        nb_preds = model_baye.predict(X_tfidf)
        nb_cm = confusion_matrix(y, nb_preds)
        nb_acc = accuracy_score(y, nb_preds)

        disp_nb = ConfusionMatrixDisplay(nb_cm, display_labels=["Human", "AI"])
        disp_nb.plot(ax=axes[i, 0], cmap='Greens', values_format='d')
        axes[i, 0].set_title(f"{name} - Naive Bayes\nAcc: {nb_acc:.3f}")

        # Random Forest
        rf_preds = rf_model.predict(X_tfidf)
        rf_cm = confusion_matrix(y, rf_preds)
        rf_acc = accuracy_score(y, rf_preds)

        disp_rf = ConfusionMatrixDisplay(rf_cm, display_labels=["Human", "AI"])
        disp_rf.plot(ax=axes[i, 1], cmap='Blues', values_format='d')
        axes[i, 1].set_title(f"{name} - Random Forest\nAcc: {rf_acc:.3f}")

    plt.tight_layout()
    plt.show()

test_sets = {
    "test_01": test_01,
    "test_02": test_04,
    "test_03": test_05
}
# ny times + midjourney
# 4: bbc + sdxl
# cnn + sdxl
plot_conf_matrices(test_sets, model_baye, rf_model_best, tfidf)

def plot_top_rf_features(rf_model, tfidf_vectorizer, top_n=20):
    """
    Plot the top N most important features (words) based on a trained Random Forest model.

    Parameters:
    - rf_model: trained RandomForestClassifier model
    - tfidf_vectorizer: the fitted TfidfVectorizer used for training
    - top_n: how many top features to display (default = 20)
    """
    import numpy as np
    import matplotlib.pyplot as plt

    # Get feature importances and names
    importances = rf_model.feature_importances_
    indices = np.argsort(importances)[-top_n:]  # indices of top features
    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())[indices]
    importance_values = importances[indices]

    # Plot
    plt.figure(figsize=(10, 6))
    plt.barh(feature_names, importance_values)
    plt.title("Top Words that Influence AI Predictions", fontweight="bold")
    plt.xlabel("Importance Score")
    plt.tight_layout()
    plt.show()

plot_top_rf_features(rf_model_best, tfidf)

